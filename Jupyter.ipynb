{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Overview\n",
    "Task 1: Use NLP techniques to analyze a collection of texts\n",
    "> In this task, you will use NLP techniques in Python to analyze texts comprising complaints regarding decisions made by the local municipality. The data are unstructured, not allowing for direct systematic analysis. In addition, the number of complaints makes overlooking the most pressing issues an intricate task. Your goal is to extract these most frequently addressed topics from the written texts, providing decision-makers with this information.\n",
    "> Task: Use NLP techniques to analyze a collection of texts\n",
    "# Dataset\n",
    "A dataset from Kaggle will be used, which can be found here: https://www.kaggle.com/datasets/sebastienverpile/consumercomplaintsdata?resource=download\n",
    "\n",
    "This data does not contain complaints of the local municipality, but instead customer reviews. This decision has been made as I couldn't find any suitable datasets on Kaggle. However, as the essence of the task remains the same – extracting main topics of interest from a collection of texts – this difference in choice of dataset should be negligible.\n",
    "## Exploration\n",
    "To get a first impression of the data, I use `pandas` to load the data from the csv file into a [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and display the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue',\n",
      "       'Consumer complaint narrative', 'Company public response', 'Company',\n",
      "       'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n",
      "       'Submitted via', 'Date sent to company', 'Company response to consumer',\n",
      "       'Timely response?', 'Consumer disputed?', 'Complaint ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/Consumer_Complaints.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, there is a total of 18 columns. Most of these won't be of interest to us for this task. The only things that are of interest are what product the complaint is about and what the content of the complaint is. This data will allow us to analyze what the main complaints for certain products are.\n",
    "\n",
    "> [!hint] Business Interest\n",
    "> This is of interest to the business side, as this enables them to individually address product's issues, based on the frequency of customer complaints mentioning them\n",
    "\n",
    "Next, let's look at what product categories there are, by printing the unique values of the respective column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mortgage' 'Credit reporting' 'Consumer Loan' 'Credit card'\n",
      " 'Debt collection' 'Student loan' 'Bank account or service'\n",
      " 'Other financial service' 'Prepaid card' 'Money transfers'\n",
      " 'Credit reporting, credit repair services, or other personal consumer reports'\n",
      " 'Payday loan' 'Checking or savings account'\n",
      " 'Money transfer, virtual currency, or money service'\n",
      " 'Credit card or prepaid card' 'Vehicle loan or lease'\n",
      " 'Payday loan, title loan, or personal loan' 'Virtual currency']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Product\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a wide range of product categories. If we want to analyze all categories, a more general solution would be of interest. One that simply takes a collection of texts, processes them, and then yields the main topics.\n",
    "\n",
    "> [!hint] Business Interest\n",
    "> This could be of interest to businesses as it would provide a solution that can be applied to any number of product lines including sub-products and services.\n",
    "## Preprocessing\n",
    "As already mentioned, not all the data is actually needed. Therefore, a new, cleaned dataset will be created. This will only contain the `product`, `sub-product`, `issue`, `sub-issue` and `consumer complaint narrative` columns. These are the only columns that actually relate to the product or the complaint and are therefore of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalizes a string by:\n",
    "    - setting everything to lowercase\n",
    "    - removing any punctuation\n",
    "    - removing any additional whitespace\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return\n",
    "    \n",
    "\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text) # remove special characters\n",
    "    cleaned_text = ' '.join(cleaned_text.split()) # remove unnecessary whitespaces\n",
    "    cleaned_text = cleaned_text.lower() # lower everything\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will use the `re` library to clean any input text and normalize it, making it easier to process later. Next, a new dataframe will be created that will store only the relevant information, which is information related to the product, the issue, and the customer complaint text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower() # lowering the column names for easier working with\n",
    "df = df[[\"product\", \"sub-product\", \"issue\", \"sub-issue\", \"consumer complaint narrative\"]] # selecting relevant data\n",
    "df.rename(columns={\"consumer complaint narrative\": \"complaint\"}, inplace=True) # shortening column name for easier working with\n",
    "\n",
    "df[\"complaint\"] = df[\"complaint\"].apply(normalize_text) # apply the function to all values and overwrite, e.g. turning \"Hello World!\" into \"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data is in a state we can work with. It should be noted that I conducted some more extensive exploration of the data which is not documented here as the task is to build a suitable NLP model. As the task (development phase/reflection phase) demands two different vectorization techniques and two semantic analysis techniques, the following development process will include some deliberate \"detours\" to fulfil these requirements. For now, let's start with the vectorization techniques. First, I will create a general function for further cleaning the text, where any filler or stop words and any censored words are removed from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the Text by:\n",
    "    - removing any stop words (such as \"the\", \"and\", etc.)\n",
    "    - removing any censors (such as \"xxxxx\")\n",
    "    \"\"\"\n",
    "    words = [word for word in str(text).split() if word.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(word for word in words if not re.match(r'^x+$', word))\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "df = df.dropna(subset=[\"complaint\"]) # remove all entries without actual complaints\n",
    "df[\"cleaned\"] = df[\"complaint\"].apply(clean_text)\n",
    "df.to_csv(\"clean_data.csv\") # create a copy of the data in case we need it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "Next, let's look into vectorization. There are many techniques, some of which make more sense than others for this particular task. For this task, the following two approaches will be developed:\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "- Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Bag of Words (BoW) vectorization\n",
    "bow = CountVectorizer()\n",
    "X_bow = bow.fit_transform(df[\"cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency-Inverse Document Frequency (TF-IDF) vectorization\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df[\"cleaned\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two vectorizers that have been fitted to the cleaned data. The difference between these two vectorizers is the following:\n",
    "BoW converts the texts into fixed-length vectors, by counting how many times a word appears in the selected text. It does not consider any importance or relevance of each word and also ignores the relations or order between words. TF-IDF is more complex and assigns weights to individual words, based on their importance in the document corpus.\n",
    "It should be noted that TF-IDF is based on BoW and therefore the two vectorizers are quite similar.\n",
    "As TF-IDF considers the importance of words by taking their frequency of a word in a document and the inverse frequency of a word across the corpus into account, it is better suited for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction\n",
    "This section will develop two semantic analysis techniques for extracting the main topics of the texts. As there are a variety of product and issue categories, this will be limited to the category with the most complaints, as that category will most likely be of highest relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Virtual currency': 16, 'Other financial service': 292, 'Money transfer, virtual currency, or money service': 684, 'Payday loan, title loan, or personal loan': 697, 'Vehicle loan or lease': 821, 'Prepaid card': 1451, 'Money transfers': 1496, 'Payday loan': 1748, 'Checking or savings account': 2142, 'Credit card or prepaid card': 3355, 'Consumer Loan': 9474, 'Student loan': 13304, 'Credit reporting, credit repair services, or other personal consumer reports': 14671, 'Bank account or service': 14888, 'Credit card': 18842, 'Credit reporting': 31592, 'Mortgage': 36582, 'Debt collection': 47915}\n"
     ]
    }
   ],
   "source": [
    "product_complaints = {}\n",
    "for product in df[\"product\"].unique():\n",
    "    product_complaints[product] = df[df[\"product\"] == product].shape[0]\n",
    "\n",
    "product_complaints = dict(sorted(product_complaints.items(), key=lambda item: item[1]))\n",
    "print(product_complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the product category `Debt collection` has the most entries, with a total of 47,915. Therefore, this will be the product category that'll be used for building the semantic analysis models. The following models will be used:\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "The goal will be to extract the 10 most prevalent topics from the text corpus, as this should be sufficient information for the business side to base decisions on.\n",
    "## LDA Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"cleaned\"].apply(lambda x: x.split()) # turn strings into tokens\n",
    "df.to_csv(\"tokenized_data.csv\") # save current state\n",
    "df = df[df[\"product\"] == \"Debt collection\"] # select subset of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# create the Dictionary and Corpus\n",
    "dictionary = corpora.Dictionary(df[\"tokens\"])\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# set number of topics\n",
    "num_topics = 10\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.024*\"called\" + 0.022*\"told\" + 0.021*\"call\" + 0.021*\"said\" + 0.021*\"would\"')\n",
      "(1, '0.015*\"nt\" + 0.015*\"pay\" + 0.014*\"would\" + 0.012*\"told\" + 0.011*\"credit\"')\n",
      "(2, '0.038*\"calls\" + 0.037*\"phone\" + 0.035*\"call\" + 0.026*\"calling\" + 0.021*\"number\"')\n",
      "(3, '0.043*\"debt\" + 0.035*\"letter\" + 0.026*\"sent\" + 0.021*\"received\" + 0.019*\"validation\"')\n",
      "(4, '0.079*\"debt\" + 0.025*\"collection\" + 0.021*\"act\" + 0.019*\"consumer\" + 0.019*\"collect\"')\n",
      "(5, '0.031*\"debt\" + 0.031*\"payment\" + 0.029*\"account\" + 0.025*\"amount\" + 0.019*\"paid\"')\n",
      "(6, '0.015*\"court\" + 0.012*\"filed\" + 0.011*\"case\" + 0.011*\"attorney\" + 0.010*\"property\"')\n",
      "(7, '0.080*\"credit\" + 0.052*\"account\" + 0.043*\"report\" + 0.036*\"debt\" + 0.025*\"collection\"')\n",
      "(8, '0.168*\"loan\" + 0.040*\"loans\" + 0.027*\"student\" + 0.020*\"payday\" + 0.013*\"school\"')\n",
      "(9, '0.065*\"bill\" + 0.043*\"medical\" + 0.039*\"insurance\" + 0.035*\"collection\" + 0.027*\"agency\"')\n"
     ]
    }
   ],
   "source": [
    "# print the top 10 topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the LDA needs to be interpreted by a human to make sense of. The interpretation of the following output is:\n",
    "- communication issues, as there are lots of references to \"call\", \"sent\", \"phone\", \"letter\", \"told\", and so on\n",
    "- issues regarding debt, as debt and payments are also more common topics\n",
    "- legal issues, as mentions of court and attorneys\n",
    "- student and medical issues\n",
    "These findings can be used to improve the service surrounding these issues.\n",
    "## NMF Development\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# setting up the vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df[\"cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the NMF model\n",
    "num_topics = 10 # as above. Added again so blocks could be run independently\n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(X)\n",
    "H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "credit report reporting removed bureaus\n",
      "\n",
      "Topic 2:\n",
      "number phone called information person\n",
      "\n",
      "Topic 3:\n",
      "debt collect collector original owe\n",
      "\n",
      "Topic 4:\n",
      "nt told pay said payment\n",
      "\n",
      "Topic 5:\n",
      "account opened closed open balance\n",
      "\n",
      "Topic 6:\n",
      "collection agency paid insurance medical\n",
      "\n",
      "Topic 7:\n",
      "letter sent validation received requested\n",
      "\n",
      "Topic 8:\n",
      "identity theft victim police result\n",
      "\n",
      "Topic 9:\n",
      "company owe contract asked knowledge\n",
      "\n",
      "Topic 10:\n",
      "calls calling day times stop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the topics\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    top_features_indices = topic.argsort()[-5:][::-1]  # Get top 5 words like with LDA\n",
    "    top_features = [feature_names[i] for i in top_features_indices]\n",
    "    print(\" \".join(top_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the NMF also needs to be interpreted by a human to make sense of. My personal interpretation here is as follows:\n",
    "- communication issues, as there are again references to \"phone\", \"called\", \"sent\", \"letter\", \"received\", \"validation\", \"requested\", and other words that relate to communication\n",
    "- account issues (see topic 5)\n",
    "- legal and identity theft issues (topic 8)\n",
    "### Output Comparison\n",
    "Both the LDA and NMF yield similar results, such as the communication and legal issues. However, there are some differences and the top 10 are in a different order. Nonetheless, both results are considered sufficient, as they give insight into the most common issues. There are some duplicates among the topics, which indicates that there are fewer than 10 main topics at hand. However, this isn't an issue as the results need to be interpreted by a human regardless."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
